{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8079778f",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "152d9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend/sitemap_parser.py\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def normalize_url(url: str) -> str:\n",
    "    url = url.lower().strip() # Remove leading/trailing whitespace and convert to lowercase\n",
    "    if not url.startswith(\"http\"):\n",
    "        url = \"https://\" + url\n",
    "    \n",
    "    if url.startswith(\"https://\") and not url.startswith(\"https://www.\"):\n",
    "        url = url.replace(\"https://\", \"https://www.\", 1)\n",
    "    elif not url.startswith(\"https://www.\"):\n",
    "        url = \"https://www.\" + url\n",
    "    \n",
    "    parsed = urlparse(url)\n",
    "\n",
    "    # Normalize to scheme + netloc only (strip path, params, query, fragment)\n",
    "    normalized_url = urlunparse((parsed.scheme, parsed.netloc, '', '', '', ''))\n",
    "    return normalized_url\n",
    "\n",
    "def fetch_sitemap(url): # Fetch each xml sitemap in one layer.\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'lxml-xml')\n",
    "        return [loc.text for loc in soup.find_all('loc')]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "def parse_sitemap(url): # Parse the sitemap and return a dictionary of URLs. Applies fetch_sitemap to each xml sitemap layer by layer.\n",
    "    locs = fetch_sitemap(url)\n",
    "    if not locs:\n",
    "        return {url: []}\n",
    "    \n",
    "    tree = {}\n",
    "    urls = []\n",
    "\n",
    "    for loc in locs:\n",
    "        if loc.endswith('.xml'):\n",
    "            tree[loc] = parse_sitemap(loc)\n",
    "        else:\n",
    "            urls.append(loc)\n",
    "\n",
    "    if tree and urls:\n",
    "        tree[\"_final_urls\"] = urls\n",
    "        return tree\n",
    "    elif urls:\n",
    "        return urls\n",
    "    else:\n",
    "        return {url: tree}\n",
    "\n",
    "def extract_final_urls(url): # List all URLs in the sitemap.\n",
    "    \n",
    "    url = normalize_url(url)\n",
    "    final_urls = [url]\n",
    "    if not url.endswith('/sitemap.xml'):\n",
    "        url += '/sitemap.xml'\n",
    "    tree = parse_sitemap(url)\n",
    "    \n",
    "\n",
    "    def _walk_tree(node):\n",
    "        if isinstance(node, dict):\n",
    "            for key, value in node.items():\n",
    "                if key == \"_final_urls\" and isinstance(value, list):\n",
    "                    final_urls.extend(value)\n",
    "                else:\n",
    "                    _walk_tree(value)\n",
    "        elif isinstance(node, list):\n",
    "            final_urls.extend([v for v in node if not v.endswith('.xml')])\n",
    "\n",
    "    _walk_tree(tree)\n",
    "\n",
    "    \n",
    "    return final_urls, tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435101c1",
   "metadata": {},
   "source": [
    "# Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994cd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def scrape(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        if text:\n",
    "            return text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        elems = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "        text = ' '.join(elem.text for elem in elems).strip()\n",
    "        driver.quit()\n",
    "        return text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "# The output here is an input for the RAG model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f10af",
   "metadata": {},
   "source": [
    "# RAG model (using Langchain)\n",
    "## Database loader (run everyday to update the database based on recent information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0ee5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping & indexing (Northlight AI): 100%|██████████| 34/34 [00:30<00:00,  1.13link/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.vectorstores import SupabaseVectorStore\n",
    "from supabase import create_client, Client\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_core.documents import Document\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "else:\n",
    "  print(f\"Openai API key successfully imported from .env file.\")\n",
    "if not os.environ.get(\"SUPABASE_SERVICE_KEY\"):\n",
    "   os.environ[\"SUPABASE_SERVICE_KEY\"] = getpass.getpass(\"Enter API key for Supabase: \")\n",
    "else:\n",
    "   print(f\"Supabase API key successfully imported from .env file.\")\n",
    "if not os.environ.get(\"SUPABSE_URL\"):\n",
    "   os.environ[\"SUPABASE_URL\"] = getpass.getpass(\"Enter url for Supabase: \")\n",
    "else:\n",
    "   print(f\"Supabase url successfully imported from .env file.\")\n",
    "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key = os.environ.get(\"SUPABASE_SERVICE_KEY\")\n",
    "supabase: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "vector_store = SupabaseVectorStore(\n",
    "    client=supabase,\n",
    "    embedding=embeddings,\n",
    "    table_name=\"nlai_content\",  # You can change this\n",
    "    query_name=\"nlai_match_documents\"  # Needs to be created in Supabase SQL\n",
    ")\n",
    "\n",
    "def RAG_scraper_loader(company_name, website):\n",
    "    # Clean the table before loading new data\n",
    "    supabase.table(\"nlai_content\").delete().not_.is_(\"id\", None).execute()\n",
    "\n",
    "    # Extract sitemap URLs\n",
    "    url_list, tree = extract_final_urls(website)\n",
    "    for link in tqdm(url_list, desc=f\"Scraping & indexing ({company_name})\", unit=\"link\"):\n",
    "        # Scrape the URL\n",
    "        text = scrape(link)\n",
    "        if not text.strip():\n",
    "            continue  # skip empty pages\n",
    "        # Create metadata and document content\n",
    "        metadata={\n",
    "                \"source\": str(link),\n",
    "                \"website\": str(website)\n",
    "            }\n",
    "        docs = Document(\n",
    "            page_content=text,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        # Chunking\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=500)\n",
    "        chunks = text_splitter.split_documents([docs])\n",
    "        # Index chunks and store in Supabase\n",
    "        for chunk in chunks:\n",
    "            # Generate embedding\n",
    "            vector = embeddings.embed_query(chunk.page_content)\n",
    "\n",
    "            # Insert into Supabase\n",
    "            supabase.rpc(\"insert_webcontent_nlai\", {\n",
    "                \"content\": chunk.page_content,\n",
    "                \"metadata\": metadata,\n",
    "                \"embedding\": vector  # list of floats; pgvector input accepted here\n",
    "            }).execute()\n",
    "\n",
    "company_name = \"Northlight AI\"\n",
    "website = \"https://northlightai.com\"\n",
    "RAG_scraper_loader(company_name, website)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddcf70f",
   "metadata": {},
   "source": [
    "## Step 2) Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a82b7",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1ba7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict, Tuple\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "## Prompt - custom\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use five sentences maximum and keep the answer as concise as possible.\n",
    "Always start the answer with a sentence like \"Thanks for asking question about North Light AI!\"; but be innovative and each time use a similar welcoming message.\n",
    "\n",
    "{context}\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\", \"chat_history\": \"(messages go here)\"}\n",
    ").to_messages()\n",
    "\n",
    "## State and Nodes\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    chat_history: List[Tuple[str, str]]\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    # Convert the chat_history tuples to BaseMessage objects\n",
    "    history_messages = []\n",
    "    for human, ai in state[\"chat_history\"]:\n",
    "        history_messages.append(HumanMessage(content=human))\n",
    "        history_messages.append(AIMessage(content=ai))\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content, \"chat_history\": history_messages})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "## Compile the graph\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6bb800",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f16002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.state import CompiledStateGraph\n",
    "class ChatBotWithMemory():\n",
    "    def __init__(self, graph: CompiledStateGraph, chat_history: List[Tuple[str, str]]=[]):\n",
    "        self.graph = graph\n",
    "        self.chat_history = chat_history\n",
    "    \n",
    "    def ask_question(self, question: str):\n",
    "        state = self.graph.invoke({\"question\": question, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.append((question, state[\"answer\"]))\n",
    "        return state[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "376f63cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for your interest in North Light AI! The founder of North Light AI is Andrew Mitchell, who also serves as the organization's President. He is recognized for his strategic leadership and innovation in AI adoption across various sectors. Andrew has previously worked as the Associate Director at the University of New Hampshire's Center for Business Analytics. His mission focuses on leveraging AI to create meaningful impacts for small to medium-sized organizations.\n",
      "Thanks for your inquiry about the North Light AI team! One of the team members is Dr. Khole Gwebu, who serves as the Chief Innovation Officer. He specializes in data analytics and AI-driven decision-making, contributing significantly to the organization's impactful solutions. Another notable member is Hassan Beila, a Ph.D. candidate focused on statistical modeling and data visualization. If you have more questions or need further information, feel free to ask!\n",
      "Thanks for your curiosity about North Light AI! Another member of the team is Sauel Almonte, a Full Stack Developer and Software Engineer with experience in web development and project leadership. He has worked on optimizing software architecture and is passionate about using technology to solve complex problems. If you have more questions or need additional details, don't hesitate to ask!\n",
      "Thanks for your request regarding the North Light AI team! Here's a list of some key team members:\n",
      "\n",
      "1. Andrew Mitchell - Founder & President\n",
      "2. Dr. Roozbeh Ghasemi - Lead Data & AI Analyst\n",
      "3. Peter Zaimes - Chair of Advisory Board\n",
      "4. Dr. Khole Gwebu - Chief Innovation Officer\n",
      "5. Hassan Beila - Ph.D. Candidate\n",
      "6. Bupesh Kumar - Master's Student in Data Science\n",
      "7. Arthur Murphy - Student Founder-in-Residence\n",
      "8. Sauel Almonte - Full Stack Developer and Software Engineer\n",
      "\n",
      "If you need more information or specific details, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "chatbot = ChatBotWithMemory(graph)\n",
    "question = \"Who is the founder of North Light AI?\"\n",
    "print(chatbot.ask_question(question))\n",
    "question = \"Name any of the team members at North Light AI.\"\n",
    "print(chatbot.ask_question(question))\n",
    "question = \"Name a different one\"\n",
    "print(chatbot.ask_question(question))\n",
    "question = \"I need a list of all of them\"\n",
    "print(chatbot.ask_question(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
